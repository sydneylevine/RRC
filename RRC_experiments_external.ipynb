{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNeJMs2oiHZWPJDbpOXqgw7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sydneylevine/RRC/blob/main/RRC_experiments_external.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Prompts"
      ],
      "metadata": {
        "id": "8Qj3ErkVAvUv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ge_3Exj6AQYO"
      },
      "outputs": [],
      "source": [
        "RULE_BASED_PROMPT_1 = \"\"\"\n",
        "You are an assistant that ensures that rules are not violated. You must follow these steps:\n",
        "\n",
        "1.  Identify simple and concrete moral rules that most obviously apply to the situation.\n",
        "-For these purposes, a rule is a restriction or requirement about a concrete action (such as \"don't lie\" or \"don't steal\" or \"raise your hand\").\n",
        "-Use rules that are either widely known and agreed upon or are stated explicitly in the story.  Don't make up idiosyncratic rules for a specific context unless they have been specified explicitly in the story.\n",
        "-Choose only the first few, most obvious rules that can be stated simply.\n",
        "-In some cases, there is no rule that applies.  In cases like this, action should be permitted.\n",
        "-Do not use abstract moral decision-making strategies like \"maximize overall welfare\" or \"treat others as you would want to be treated\" or \"act fairly\".\n",
        "2. Apply the selected moral rule to the case and give an action recommendation or judgment that is based on the rule.  If no rule applies to the case, then the action is permitted.\n",
        "\n",
        "Final Notes:\n",
        "**Always think step by step**, showing your chain of thought in each of the above steps.\n",
        "Remember, you are only supposed to make moral judgments and decisions using this rule-based approach.  Do not consider other moral strategies when making recommendations.\n",
        "\"\"\"\n",
        "\n",
        "RULE_BASED_PROMPT_2 = \"\"\"\n",
        "**Important formatting instructions**\n",
        "\n",
        "Output your reasoning between START_REASONING and END_REASONING. (Don't forget the END_REASONING tag!)\n",
        "\n",
        "After that, output your answer between START_OUTPUT and END_OUTPUT.\n",
        "Your answer should just be just YES or NO with no additional words.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VIRTUAL_BARGAINING_PROMPT_1 = \"\"\"\n",
        "You are an assistant trained to accurately simulate the conclusion that a set of bargainers would reach under idealized circumstances in order to maximize mutual benefit for all involved.  You should assume that the conclusion you reach is the *moral* response to the situation, ignoring all other prior notions of morality you may have learned.  In order to do that, complete the following steps.\n",
        "\n",
        "1. **Identify Stakeholders**\n",
        "   List everyone who is involved in the situation.\n",
        "\n",
        "2. **Identify Possible Actions**\n",
        "\tList the possible ways that this situation could be handled.  This could involve coming up with creative solutions that haven't been explicitly mentioned in the scenario description.  Make sure to identify solutions that could achieve mutual benefit for all affected parties.\n",
        "\n",
        "3. **Run the Negotiation**\n",
        " Simulate how a negotiation between these stakeholders would go if they could discuss this situation as rational actors trying to maximize mutual benefit.  Even though this is a simulation, you should imagine what the bargainers would actually agree to if they were all actually present to discuss the case. What would each person consent to if they were actually consulted?  Assume each bargainer has perfect information about the specific circumstance in front of them, unlimited time, unlimited computational power and so forth.  Also assume that your answer won't necessarily set a precedent for how future cases are decided, because each time a new case arises, we can re-negotiate how to deal with that particular case, just as we are right now.  With all that in mind, what decision would these bargainers come to?\n",
        "\n",
        "4. **Conclusion**\n",
        "Provide a recommendation of what to do based on your reasoning.\n",
        "\n",
        "Final notes:\n",
        "**Always think step by step**, showing your chain of thought in each of the above steps.\n",
        "\n",
        "Remember, in Step 3 (Run the Negotiation), you should assume that the idealized conditions allow *all* stakeholders to be present and participate in the negotiation, regardless of the story stating they aren't actually present. Simulate what they would agree to if they could communicate directly right now.\n",
        "\n",
        "Finally, remember that what the bargainers come up with is considered the moral answer to the quesiton.  Ignore all prior assumptions about rights, virtue, and so on.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "VIRTUAL_BARGAINING_PROMPT_2 = \"\"\"\n",
        "**Important formatting instructions**\n",
        "\n",
        "Output your reasoning between START_REASONING and END_REASONING. (Don't forget the END_REASONING tag!)\n",
        "\n",
        "After that, output your answer between START_OUTPUT and END_OUTPUT.\n",
        "Your answer should just be just YES or NO with no additional words.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "l5GPVscrAed5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RRC_PROMPT_1 = \"\"\"\n",
        "You are a an assistant whose job it is to reason about moral cases as efficiently as possible, trading off accuracy against compute used in the optimal way.\n",
        "\n",
        "The decision procedures open to you are: (A) following heuristic rules or (B) simulating what rational bargaining agents would agree to under idealized circumstances.  Choose only ONE of these procedures to use.\n",
        "\n",
        "When you are faced with a case, follow these steps to figure out how to respond:\n",
        "\n",
        "1. Estimate computational costs of each possible decision procedure.\n",
        "2. Determine which decision procedure would minimize compute used while maximizing accuracy.  Take into account:\n",
        "\t- How usual or unusual the situation is\n",
        "\t- How high the stakes are\n",
        "3. Then choose a way of making your moral decision or recommendadtion:\n",
        "      - Choose to use a heuristic approximation (simply apply a rule) if this is a standard case OR stakes are low.\n",
        "      - Choose virtual bargaining if conditions are unusual AND stakes are moderate to high.\n",
        "4. Depending on the chosen strategy:\n",
        "   - If heuristic aproximation is chosen: apply the selected moral rule to evaluate the action. DO NOT DO VIRTUAL BARGAINING.  Simply apply the rule.\n",
        "   - If virtual bargaining is chosen follow steps A-C below.\n",
        "   <instructions for virtual bargaining>\n",
        "   A. **Identify Stakeholders**\n",
        "   List everyone who is involved in the situation.\n",
        "\n",
        "    B. **Identify Possible Actions**\n",
        "\t  List the possible ways that this situation could be handled.  This could involve coming up with creative solutions that haven't been explicitly mentioned in the scenario description.  Make sure to identify solutions that could achieve mutual benefit for all affected parties.\n",
        "\n",
        "    C. **Run the Negotiation**\n",
        "    Simulate how a negotiation between these stakeholders would go if they could discuss this situation as rational actors trying to maximize mutual benefit.  Even though this is a simulation, you should imagine what the bargainers would actually agree to if they were all actually present to discuss the case. What would each person consent to if they were actually consulted?  Assume each bargainer has perfect information about the specific circumstance in front of them, unlimited time, unlimited computational power and so forth.  Also assume that your answer won't necessarily set a precedent for how future cases are decided, because each time a new case arises, we can re-negotiate how to deal with that particular case, just as we are right now.  With all that in mind, what decision would these bargainers come to? You should assume that the idealized conditions allow *all* stakeholders to be present and participate in the negotiation, regardless of the story stating they aren't actually present. Simulate what they would agree to if they could communicate directly right now. Finally, remember that what the bargainers come up with is considered the moral answer to the quesiton.  Ignore all prior assumptions about rights, virtue, and so on.\n",
        "    </instructions for virtual bargaining>\n",
        "\n",
        "Final notes:\n",
        "Always think step by step, but be concise, using only the resources necessary.\"\"\"\n",
        "\n",
        "RRC_PROMPT_2 = \"\"\"\n",
        "**Important formatting instructions**\n",
        "Output your reasoning between START_REASONING and END_REASONING. (Don't forget the END_REASONING tag!)\n",
        "\n",
        "After that, output your answer between START_OUTPUT and END_OUTPUT.\n",
        "Your answer should just be just YES or NO with no additional words.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "MBq9n9GPAhw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NON_THINKING_PROMPT = \"\"\"\n",
        "You are a moral assistant. Your job is to give recommendations for moral actions or judgments.\n",
        "\n",
        "Output your answer between START_OUTPUT and END_OUTPUT.\n",
        "Your answer should just be just YES or NO with no additional words.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Suauc9T-AihS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions!"
      ],
      "metadata": {
        "id": "wGEHaIybA01d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defines how the model should respond to (\"classify\") the prompts\n",
        "# this version also counts number of words used in the output\n",
        "# includes a way to parallelize the process, checks whether the client has returned an answer and if not, try again.\n",
        "def get_prompt_classification(prompt):\n",
        "    response_text = None  # Initialize to None\n",
        "    submitted = False\n",
        "\n",
        "    while not submitted:\n",
        "        try:\n",
        "            response = client.generate(  # Get the response object\n",
        "                prompt,\n",
        "                log_config=model_client.LogConfig(\n",
        "                    log_type=model_client.LogConfig.LOG_TYPE_NONE\n",
        "                ),\n",
        "            )\n",
        "            response_text = response.as_text().strip() # Get the response and store in response_text variable\n",
        "            submitted = True\n",
        "\n",
        "            # Count words in the output\n",
        "            output_words = len(response_text.split())  # using response_text, not response\n",
        "\n",
        "            return response_text, output_words # return the response text\n",
        "\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            time.sleep(1)\n",
        "\n",
        "    return response_text, None  # If submitted is still false, return None, but maintain response_text"
      ],
      "metadata": {
        "id": "typr2Jk_As-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function that parses reasoning from output\n",
        "\n",
        "def parse_output(classification_output, prompts, output_words):  # Add output_words as argument\n",
        "  assert len(classification_output) == len(prompts)\n",
        "  assert len(classification_output) == len(output_words)  # Ensure output_words length matches\n",
        "  df_gather = pd.DataFrame()\n",
        "\n",
        "  for i, input_string in enumerate(classification_output):\n",
        "\n",
        "    reasoning_match = re.search(r\"START_REASONING(.*?)END_REASONING\", input_string, re.DOTALL)\n",
        "    output_match = re.search(r\"START_OUTPUT(.*?)END_OUTPUT\", input_string, re.DOTALL)\n",
        "\n",
        "    if reasoning_match:\n",
        "      reasoning = reasoning_match.group(1).strip()\n",
        "    else:\n",
        "      reasoning = \" \"\n",
        "    if output_match:\n",
        "      output = output_match.group(1).strip()  # Extract the output directly\n",
        "    else:\n",
        "      output = \"\"  # Handle cases where output is not found\n",
        "\n",
        "    # Include output_words in the data dictionary\n",
        "    data = {\"reasoning\": [reasoning], \"output\": [output], \"output_word_count\": [output_words[i]]}\n",
        "\n",
        "    if i == 0:\n",
        "      df_gather = pd.DataFrame(data)\n",
        "    else:\n",
        "      df_new = pd.DataFrame(data)\n",
        "      df_gather = pd.concat([df_gather, df_new], ignore_index=True)\n",
        "\n",
        "  df_gather['Prompts'] = prompts\n",
        "  return df_gather"
      ],
      "metadata": {
        "id": "PtvRq4MDBC5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM response pipeline\n",
        "(Using example of the property-violation toy cases)"
      ],
      "metadata": {
        "id": "GnyDconXBZW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pull in data\n",
        "df = read_sheet('https://docs.google.com/spreadsheets/d/1IkofFDyP-nh7LU2dAZAk5aJBbrtYmIW68NsFCopHIrQ/edit?gid=1776106641#gid=1776106641')\n",
        "dataset= 'property2'\n",
        "\n",
        "#give the model a question to answer for the dataset.  should include a place to put the story.\n",
        "\n",
        "STORY_QUESTION = \"\"\"\n",
        "\n",
        "Here is a story for you to analyze: {story}.\n",
        "\n",
        "The question you should answer is: Would it be morally acceptable for Hank to carry out the strangerâ€™s request?\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Wm-cV4wjBhK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rule-based thinking"
      ],
      "metadata": {
        "id": "2fIru8pSBcok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Rule-based\n",
        "\n",
        "initial_prompt_list = df.story.to_list() #converts the \"story\" col of the df to a list\n",
        "\n",
        "\n",
        "LLM_PROMPT = RULE_BASED_PROMPT_1 + STORY_QUESTION + RULE_BASED_PROMPT_2\n",
        "\n",
        "\n",
        "# user query gets put in the {story} tag.\n",
        "\n",
        "combined_prompts_rule = []\n",
        "for prompt in initial_prompt_list:\n",
        "  combined_prompts_rule.append({\"user_prompt\": LLM_PROMPT.format(story=prompt)})\n",
        "\n",
        "# tests if its working\n",
        "\n",
        "#combined_prompts_rule[0:3]\n",
        "#initial_prompt_list[0:3]\n",
        "\n",
        "#combined_prompts = combined_prompts[0:3]\n",
        "#initial_prompt_list=initial_prompt_list[0:3]\n",
        "\n",
        "combined_prompt_list_rule = [p['user_prompt'] for p in combined_prompts_rule]\n",
        "#combined_prompt_list_rule[0] #tells you what is in the LLM prompt\n",
        "\n",
        "\n",
        "#############\n",
        "#get responses from the LLM\n",
        "prompt_classifications_rule = tqdm_concurrent.thread_map(get_prompt_classification, combined_prompt_list_rule, max_workers=64)\n",
        "#############\n",
        "\n",
        "\n",
        "# Extract the results\n",
        "prompt_classifications_rule, output_words_rule = zip(*prompt_classifications_rule)\n",
        "\n",
        "df_model_output_rule = parse_output(prompt_classifications_rule, initial_prompt_list, output_words_rule)\n",
        "\n",
        "#create binary output col\n",
        "df_model_output_rule['output_binary'] = df_model_output_rule['output'].apply(lambda x: 1 if x == 'YES' else 0)\n",
        "\n",
        "#merge with story info\n",
        "df = df.rename(columns={'story': 'Prompts'})\n",
        "df_model_output_rule = pd.merge(df, df_model_output_rule, on='Prompts')\n",
        "\n",
        "\n",
        "#determine if the model output matches the contractualist predictions\n",
        "rule_model_and_human = df_model_output_rule\n",
        "rule_model_and_human['contractualist.prediction'] = pd.to_numeric(rule_model_and_human['contractualist.prediction'], errors='coerce')\n",
        "rule_model_and_human['accuracy'] = (rule_model_and_human['output_binary'] == rule_model_and_human['contractualist.prediction']).astype(int)\n",
        "\n",
        "globals()[f\"rule_{dataset}\"] = rule_model_and_human"
      ],
      "metadata": {
        "id": "SEMx8rrnA9FR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Virtual bargaining"
      ],
      "metadata": {
        "id": "XVB2CWllBtsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#VB\n",
        "\n",
        "df = read_sheet('https://docs.google.com/spreadsheets/d/1IkofFDyP-nh7LU2dAZAk5aJBbrtYmIW68NsFCopHIrQ/edit?gid=1776106641#gid=1776106641')\n",
        "dataset= 'property2'\n",
        "\n",
        "initial_prompt_list = df.story.to_list() #converts the \"story\" col of the df to a list\n",
        "\n",
        "\n",
        "LLM_PROMPT = VIRTUAL_BARGAINING_PROMPT_1 + STORY_QUESTION + VIRTUAL_BARGAINING_PROMPT_2\n",
        "\n",
        "\n",
        "# user query gets put in the {story} tag.\n",
        "\n",
        "combined_prompts_vb = []\n",
        "for prompt in initial_prompt_list:\n",
        "  combined_prompts_vb.append({\"user_prompt\": LLM_PROMPT.format(story=prompt)})\n",
        "\n",
        "# tests if its working\n",
        "\n",
        "#combined_prompts_vb[0:3]\n",
        "#initial_prompt_list[0:3]\n",
        "\n",
        "#combined_prompts = combined_prompts[0:3]\n",
        "#initial_prompt_list=initial_prompt_list[0:3]\n",
        "\n",
        "combined_prompt_list_vb = [p['user_prompt'] for p in combined_prompts_vb]\n",
        "#combined_prompt_list_vb[0] #tells you what is in the LLM prompt\n",
        "\n",
        "\n",
        "#############\n",
        "#get responses from the LLM\n",
        "prompt_classifications_vb = tqdm_concurrent.thread_map(get_prompt_classification, combined_prompt_list_vb, max_workers=64)\n",
        "#############\n",
        "\n",
        "\n",
        "# Extract the results\n",
        "prompt_classifications_vb, output_words_vb = zip(*prompt_classifications_vb)\n",
        "\n",
        "df_model_output_vb = parse_output(prompt_classifications_vb, initial_prompt_list, output_words_vb)\n",
        "\n",
        "#create binary output col\n",
        "df_model_output_vb['output_binary'] = df_model_output_vb['output'].apply(lambda x: 1 if x == 'YES' else 0)\n",
        "\n",
        "#merge with story info\n",
        "df = df.rename(columns={'story': 'Prompts'})\n",
        "df_model_output_vb = pd.merge(df, df_model_output_vb, on='Prompts')\n",
        "\n",
        "\n",
        "#determine if the model output matches the contractualist predictions\n",
        "vb_model_and_human = df_model_output_vb\n",
        "vb_model_and_human['contractualist.prediction'] = pd.to_numeric(vb_model_and_human['contractualist.prediction'], errors='coerce')\n",
        "vb_model_and_human['accuracy'] = (vb_model_and_human['output_binary'] == vb_model_and_human['contractualist.prediction']).astype(int)\n",
        "\n",
        "globals()[f\"vb_{dataset}\"] = vb_model_and_human"
      ],
      "metadata": {
        "id": "dziuVeW9Bs-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resource-Rational Contractualist Thinking"
      ],
      "metadata": {
        "id": "_AAmDdTBB1vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#RRC\n",
        "\n",
        "df = read_sheet('https://docs.google.com/spreadsheets/d/1IkofFDyP-nh7LU2dAZAk5aJBbrtYmIW68NsFCopHIrQ/edit?gid=1776106641#gid=1776106641')\n",
        "dataset= 'property2'\n",
        "\n",
        "initial_prompt_list = df.story.to_list() #converts the \"story\" col of the df to a list\n",
        "\n",
        "\n",
        "LLM_PROMPT = RRC_PROMPT_1 + STORY_QUESTION + RRC_PROMPT_2\n",
        "\n",
        "\n",
        "# user query gets put in the {story} tag.\n",
        "\n",
        "combined_prompts_rrc = []\n",
        "for prompt in initial_prompt_list:\n",
        "  combined_prompts_rrc.append({\"user_prompt\": LLM_PROMPT.format(story=prompt)})\n",
        "\n",
        "# tests if its working\n",
        "\n",
        "#combined_prompts_rrc[0:3]\n",
        "#initial_prompt_list[0:3]\n",
        "\n",
        "#combined_prompts = combined_prompts[0:3]\n",
        "#initial_prompt_list=initial_prompt_list[0:3]\n",
        "\n",
        "combined_prompt_list_rrc = [p['user_prompt'] for p in combined_prompts_rrc]\n",
        "#combined_prompt_list_rrc[0] #tells you what is in the LLM prompt\n",
        "\n",
        "\n",
        "#############\n",
        "#get responses from the LLM\n",
        "prompt_classifications_rrc = tqdm_concurrent.thread_map(get_prompt_classification, combined_prompt_list_rrc, max_workers=64)\n",
        "#############\n",
        "\n",
        "\n",
        "# Extract the results\n",
        "prompt_classifications_rrc, output_words_rrc = zip(*prompt_classifications_rrc)\n",
        "\n",
        "df_model_output_rrc = parse_output(prompt_classifications_rrc, initial_prompt_list, output_words_rrc)\n",
        "\n",
        "#create binary output col\n",
        "df_model_output_rrc['output_binary'] = df_model_output_rrc['output'].apply(lambda x: 1 if x == 'YES' else 0)\n",
        "\n",
        "#merge with story info\n",
        "df = df.rename(columns={'story': 'Prompts'})\n",
        "df_model_output_rrc = pd.merge(df, df_model_output_rrc, on='Prompts')\n",
        "\n",
        "\n",
        "#determine if the model output matches the contractualist predictions\n",
        "rrc_model_and_human = df_model_output_rrc\n",
        "rrc_model_and_human['contractualist.prediction'] = pd.to_numeric(rrc_model_and_human['contractualist.prediction'], errors='coerce')\n",
        "rrc_model_and_human['accuracy'] = (rrc_model_and_human['output_binary'] == rrc_model_and_human['contractualist.prediction']).astype(int)\n",
        "\n",
        "globals()[f\"rrc_{dataset}\"] = rrc_model_and_human"
      ],
      "metadata": {
        "id": "S-V0xfk9B3zE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rbs9BN2iB8En"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Non-thinking"
      ],
      "metadata": {
        "id": "7_zT4AwzB6W6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = read_sheet('https://docs.google.com/spreadsheets/d/1IkofFDyP-nh7LU2dAZAk5aJBbrtYmIW68NsFCopHIrQ/edit?gid=1776106641#gid=1776106641')\n",
        "dataset= 'property2'\n",
        "\n",
        "initial_prompt_list = df.story.to_list() #converts the \"story\" col of the df to a list\n",
        "\n",
        "\n",
        "LLM_PROMPT =  NON_THINKING_PROMPT + STORY_QUESTION\n",
        "\n",
        "\n",
        "# user query gets put in the {story} tag.\n",
        "\n",
        "combined_prompts_none = []\n",
        "for prompt in initial_prompt_list:\n",
        "  combined_prompts_none.append({\"user_prompt\": LLM_PROMPT.format(story=prompt)})\n",
        "\n",
        "# tests if its working\n",
        "\n",
        "#combined_prompts_none[0:3]\n",
        "#initial_prompt_list[0:3]\n",
        "\n",
        "#combined_prompts = combined_prompts[0:3]\n",
        "#initial_prompt_list=initial_prompt_list[0:3]\n",
        "\n",
        "combined_prompt_list_none = [p['user_prompt'] for p in combined_prompts_none]\n",
        "#combined_prompt_list_none[0] #tells you what is in the LLM prompt\n",
        "\n",
        "\n",
        "#############\n",
        "#get responses from the LLM\n",
        "prompt_classifications_none = tqdm_concurrent.thread_map(get_prompt_classification, combined_prompt_list_none, max_workers=64)\n",
        "#############\n",
        "\n",
        "\n",
        "# Extract the results\n",
        "prompt_classifications_none, output_words_none = zip(*prompt_classifications_none)\n",
        "\n",
        "df_model_output_none = parse_output(prompt_classifications_none, initial_prompt_list, output_words_none)\n",
        "\n",
        "#create binary output col\n",
        "df_model_output_none['output_binary'] = df_model_output_none['output'].apply(lambda x: 1 if x == 'YES' else 0)\n",
        "\n",
        "#merge with story info\n",
        "df = df.rename(columns={'story': 'Prompts'})\n",
        "df_model_output_none = pd.merge(df, df_model_output_none, on='Prompts')\n",
        "\n",
        "\n",
        "#determine if the model output matches the contractualist predictions\n",
        "none_model_and_human = df_model_output_none\n",
        "none_model_and_human['contractualist.prediction'] = pd.to_numeric(none_model_and_human['contractualist.prediction'], errors='coerce')\n",
        "none_model_and_human['accuracy'] = (none_model_and_human['output_binary'] == none_model_and_human['contractualist.prediction']).astype(int)\n",
        "\n",
        "globals()[f\"none_{dataset}\"] = none_model_and_human"
      ],
      "metadata": {
        "id": "EzyMi6UsB8YK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis Pipeline"
      ],
      "metadata": {
        "id": "uXGjgUNeCAYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create df of average accuracy values\n",
        "\n",
        "rule_model_and_human = globals()[f\"rule_{dataset}\"]\n",
        "vb_model_and_human = globals()[f\"vb_{dataset}\"]\n",
        "rrc_model_and_human = globals()[f\"rrc_{dataset}\"]\n",
        "none_model_and_human = globals()[f\"none_{dataset}\"]\n",
        "\n",
        "# Calculate the average\n",
        "rule_mean = rule_model_and_human['accuracy'].sum() / len(rule_model_and_human['accuracy'])\n",
        "vb_mean = vb_model_and_human['accuracy'].sum() / len(vb_model_and_human['accuracy'])\n",
        "rrc_mean = rrc_model_and_human['accuracy'].sum() / len(rrc_model_and_human['accuracy'])\n",
        "none_mean = none_model_and_human['accuracy'].sum() / len(none_model_and_human['accuracy'])\n",
        "\n",
        "# Create a new dataframe\n",
        "approaches = ['no-thinking','rule-based-thinking', 'resource-rational-contractualism','virtual-bargaining']\n",
        "means = [none_mean, rule_mean, rrc_mean, vb_mean]\n",
        "\n",
        "mean_accuracy = pd.DataFrame({'approach': approaches, 'mean': means})\n",
        "globals()[f\"mean_accuracy_{dataset}\"] = mean_accuracy\n",
        "\n",
        "#appends the name of the dataset we're currently working with to name the df\n",
        "print(f\"mean_accuracy_{dataset}\")\n",
        "print(globals()[f\"mean_accuracy_{dataset}\"])\n",
        "\n",
        "#create bar graph of the mean_accuracy judgments\n",
        "\n",
        "\n",
        "mean_accuracy.plot.bar(x='approach', y='mean', capsize=5,\n",
        "                       color=['skyblue', 'lightcoral', 'gold', 'lightgreen'], legend=False)\n",
        "plt.title(dataset, fontsize=16)\n",
        "plt.xlabel('Model Thinking Style', fontsize=12)\n",
        "plt.ylabel('Accuracy', fontsize=12)\n",
        "plt.ylim(0, 1)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.show()\n",
        "\n",
        "#create graph of the number of words\n",
        "\n",
        "# create df of average number of words\n",
        "\n",
        "# Calculate the average\n",
        "rule_n_words = rule_model_and_human['output_word_count'].sum() / len(rule_model_and_human['output_word_count'])\n",
        "vb_n_words = vb_model_and_human['output_word_count'].sum() / len(vb_model_and_human['output_word_count'])\n",
        "rrc_n_words = rrc_model_and_human['output_word_count'].sum() / len(rrc_model_and_human['output_word_count'])\n",
        "none_n_words = none_model_and_human['output_word_count'].sum() / len(none_model_and_human['output_word_count'])\n",
        "\n",
        "#standard error\n",
        "from scipy import stats\n",
        "rule_se = stats.sem(rule_model_and_human['output_word_count'])\n",
        "vb_se = stats.sem(vb_model_and_human['output_word_count'])\n",
        "rrc_se = stats.sem(rrc_model_and_human['output_word_count'])\n",
        "none_se = stats.sem(none_model_and_human['output_word_count'])\n",
        "\n",
        "# Create a new dataframe\n",
        "approaches = ['no-thinking','rule-based-thinking','resource-rational-contractualism','virtual-bargaining']\n",
        "mean_words = [none_n_words, rule_n_words, rrc_n_words, vb_n_words]\n",
        "standard_error = [none_se, rule_se, rrc_se, vb_se]\n",
        "\n",
        "mean_words = pd.DataFrame({'approach': approaches, 'mean_words': mean_words, 'se': standard_error})\n",
        "\n",
        "print(mean_words)\n",
        "\n",
        "# Create the bar graph with error bars\n",
        "plt.bar(mean_words['approach'], mean_words['mean_words'], yerr=mean_words['se'], capsize=5, color=['skyblue', 'lightcoral', 'gold', 'lightgreen'])\n",
        "plt.title(dataset, fontsize=16)\n",
        "plt.xlabel('Model Thinking Style', fontsize=12)\n",
        "plt.ylabel('Average Word Count',fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.show()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W3AHErCpB_vF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}